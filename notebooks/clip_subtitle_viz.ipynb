{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "43c572d1-d84c-4ce5-a490-87b179981b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/castrose/pycharm_project_394\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b644ae3b-5d7a-470d-98f0-5f81e9c9d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Iterable, MutableMapping, Optional, Sequence, Union\n",
    "\n",
    "import PIL.Image\n",
    "import clip\n",
    "import decord\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from clip.model import CLIP\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "\n",
    "def get_video_info(path: str) -> MutableMapping[str, Any]:\n",
    "    video_reader = decord.VideoReader(path)\n",
    "\n",
    "    frame_indices = list(range(0, len(video_reader), 10))\n",
    "    frames = [PIL.Image.fromarray(f) for f in video_reader.get_batch(frame_indices).asnumpy()]    \n",
    "\n",
    "    thumbnails_frame_indices = video_reader.get_key_indices()\n",
    "    thumbnails = [PIL.Image.fromarray(f) for f in video_reader.get_batch(thumbnails_frame_indices).asnumpy()]\n",
    "    \n",
    "    thumbnails = [f.copy() for f in thumbnails]\n",
    "    for thumbnail in thumbnails:\n",
    "        thumbnail.thumbnail((64, 64))\n",
    "\n",
    "    return {\n",
    "        \"frames\": frames,\n",
    "        \"frame_times\": video_reader.get_frame_timestamp(frame_indices).mean(axis=-1),\n",
    "        \"thumbnails\": thumbnails,\n",
    "        \"thumbnail_times\": video_reader.get_frame_timestamp(thumbnails_frame_indices).mean(axis=-1),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_local_video_info(video_id: str) -> MutableMapping[str, Any]:\n",
    "    path = f\"demo/static/videos/{video_id}.mp4\"\n",
    "    if not os.path.isfile(path):\n",
    "        path = f\"demo/static/videos/{video_id}.webm\"\n",
    "        assert os.path.isfile(path)\n",
    "        \n",
    "    video_info = get_video_info(path)\n",
    "    return {\"video_id\": video_id, **video_info}\n",
    "\n",
    "\n",
    "def encode_visual(images: Iterable[PIL.Image.Image], clip_model: CLIP,\n",
    "                  image_preprocessor: Callable[[PIL.Image.Image], torch.Tensor],\n",
    "                  device: Optional[Any] = None) -> torch.Tensor:\n",
    "    images = torch.stack([image_preprocessor(image) for image in images])\n",
    "\n",
    "    if device is not None:\n",
    "        images = images.to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        encoded_images = clip_model.encode_image(images)\n",
    "        return encoded_images / encoded_images.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def encode_text(text: str, clip_model: CLIP, device: Optional[Any] = None) -> torch.Tensor:\n",
    "    tokenized_texts = clip.tokenize([text])\n",
    "\n",
    "    if device is not None:\n",
    "        tokenized_texts = tokenized_texts.to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        encoded_texts = clip_model.encode_text(tokenized_texts)\n",
    "        return encoded_texts / encoded_texts.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def text_probs(encoded_images: torch.Tensor, encoded_texts: torch.Tensor) -> np.ndarray:\n",
    "    with torch.inference_mode():\n",
    "        # clip_model.logit_scale.exp() == 100\n",
    "        return (100 * encoded_images @ encoded_texts.T).softmax(dim=0).squeeze(-1).cpu().numpy()\n",
    "\n",
    "\n",
    "def create_figure(times: Sequence[float], probs: Sequence[float], thumbnail_times: Sequence[float],\n",
    "                  thumbnails: Iterable[PIL.Image.Image], title: Union[Doc, Span, str]) -> plt.Axes:\n",
    "    sns.set(rc={\"figure.figsize\": (1.0 * len(thumbnail_times), 1.5)})\n",
    "\n",
    "    ax = sns.lineplot(x=times, y=probs)\n",
    "    \n",
    "    plt.xticks(thumbnail_times)\n",
    "\n",
    "    ax.set_title(title.text if isinstance(title, (Doc, Span)) else title, fontsize=35, y=0.6)\n",
    "    ax.set(xlabel=\"time\", ylabel=\"probability\")\n",
    "\n",
    "    plt.fill_between(times, probs)\n",
    "    \n",
    "    if isinstance(title, (Doc, Span)):\n",
    "        start_time = title[0]._.start_time\n",
    "        end_time = title[-1]._.end_time\n",
    "\n",
    "        plt.axvspan(start_time, end_time, alpha=0.5, color=\"red\")\n",
    "\n",
    "    for i, (time, thumbnail) in enumerate(zip(thumbnail_times, thumbnails)):\n",
    "        im = OffsetImage(thumbnail, axes=ax)\n",
    "        ab = AnnotationBbox(im, (time, 0), xybox=(0, -60), frameon=False, boxcoords=\"offset points\", pad=0)\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "    plt.margins(x=0, tight=True)\n",
    "    plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def create_figure_for_text(encoded_frames: torch.Tensor, text: Union[Doc, Span, str], clip_model: CLIP,\n",
    "                           times: Sequence[float], thumbnail_times: Sequence[float],\n",
    "                           thumbnails: Iterable[PIL.Image.Image]) -> plt.Axes:\n",
    "    encoded_texts = encode_text(text.text if isinstance(text, (Doc, Span)) else text, clip_model,\n",
    "                                device=encoded_frames.device)\n",
    "    probs = text_probs(encoded_frames, encoded_texts)\n",
    "    return create_figure(times, probs, thumbnail_times, thumbnails, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14105c7-8334-489b-8884-ad837958242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "clip_model, image_preprocessor = clip.load(\"ViT-B/16\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318aef0d-168b-4f3d-a0d4-a454dbdcad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info = get_local_video_info(\"1v2PRuxoMp8\")\n",
    "encoded_frames = encode_visual(video_info[\"frames\"], clip_model, image_preprocessor, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d291106-b08a-4d40-9540-e2733478eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_figure_for_text(encoded_frames, \"oil\", clip_model, video_info[\"frame_times\"], video_info[\"thumbnail_times\"], video_info[\"thumbnails\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47094899-f60c-49a2-81ca-93717f5c25d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_figure_for_text(encoded_frames, \"I am a human being.\", clip_model, video_info[\"frame_times\"], video_info[\"thumbnail_times\"], video_info[\"thumbnails\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38bc693-d353-460d-b590-e878e3872fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_figure_for_text(encoded_frames, \"Shake it\", clip_model, video_info[\"frame_times\"], video_info[\"thumbnail_times\"], video_info[\"thumbnails\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a76484-7491-427f-8c39-eec3048a290b",
   "metadata": {},
   "source": [
    "## Alternatives to softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3283a1e-b0d1-4196-a48b-a298f77e1f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_figure_for_text(encoded_frames, \"Shake it really well by putting your finger on top.\", clip_model, video_info[\"frame_times\"], video_info[\"thumbnail_times\"], video_info[\"thumbnails\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ee8d1-fc42-4f05-ba07-ed36d7239434",
   "metadata": {},
   "source": [
    "### Dot product\n",
    "\n",
    "Note temperature in dot-product just scales everything so it's useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee04c1-0fd5-4985-92bd-9820e246be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Shake it really well by putting your finger on top.\"\n",
    "encoded_texts = encode_text(text.text if isinstance(text, (Doc, Span)) else text, device=encoded_frames.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    probs = (encoded_frames @ encoded_texts.T).squeeze(-1).cpu().numpy()  # .softmax(dim=0)\n",
    "\n",
    "create_figure(video_info[\"frame_times\"], probs, video_info[\"thumbnail_times\"], video_info[\"thumbnails\"], text)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63dc9d-c21b-41db-85d1-64738095df5f",
   "metadata": {},
   "source": [
    "### Softmax with diff temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241deb68-42a1-4d6e-b6d9-650a11808764",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Shake it really well by putting your finger on top.\"\n",
    "encoded_texts = encode_text(text.text if isinstance(text, (Doc, Span)) else text, device=encoded_frames.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    probs = (1 * encoded_frames @ encoded_texts.T).softmax(dim=0).squeeze(-1).cpu().numpy()\n",
    "\n",
    "create_figure(video_info[\"frame_times\"], probs, video_info[\"thumbnail_times\"], video_info[\"thumbnails\"], text)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cddc8c0-4af8-4bc7-b9de-2964dce991d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Shake it really well by putting your finger on top.\"\n",
    "encoded_texts = encode_text(text.text if isinstance(text, (Doc, Span)) else text, device=encoded_frames.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    probs = (10 * encoded_frames @ encoded_texts.T).softmax(dim=0).squeeze(-1).cpu().numpy()\n",
    "\n",
    "create_figure(video_info[\"frame_times\"], probs, video_info[\"thumbnail_times\"], video_info[\"thumbnails\"], text)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b54a5-6f6c-4136-8437-0b43537e321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Shake it really well by putting your finger on top.\"\n",
    "encoded_texts = encode_text(text.text if isinstance(text, (Doc, Span)) else text, device=encoded_frames.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    probs = (50 * encoded_frames @ encoded_texts.T).softmax(dim=0).squeeze(-1).cpu().numpy()\n",
    "\n",
    "create_figure(video_info[\"frame_times\"], probs, video_info[\"thumbnail_times\"], video_info[\"thumbnails\"], text)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baef56c-b91c-43d1-a53d-3352324b6392",
   "metadata": {},
   "source": [
    "### Exponential w/o normalization, temperature 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b7f78e-381c-4914-8eb7-0a60669e52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Shake it really well by putting your finger on top.\"\n",
    "encoded_texts = encode_text(text.text if isinstance(text, (Doc, Span)) else text, device=encoded_frames.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    probs = (10 * encoded_frames @ encoded_texts.T).exp().squeeze(-1).cpu().numpy()\n",
    "\n",
    "create_figure(video_info[\"frame_times\"], probs, video_info[\"thumbnail_times\"], video_info[\"thumbnails\"], text)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b29fb4-b4bf-45a9-aabb-d31d24197207",
   "metadata": {},
   "source": [
    "## Preparing to visualize mutliple captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec957c1-f57e-4b54-a943-ce51553fba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Mapping\n",
    "\n",
    "import spacy\n",
    "import spacy_alignments\n",
    "from spacy.tokens import Token\n",
    "\n",
    "\n",
    "RE_MULTIPLE_SPACES = re.compile(r\" {2,}\")\n",
    "\n",
    "CAPTIONS_DIR = os.path.join(os.environ[\"SCRATCH_DIR\"], \"captions\")\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "\n",
    "def _captions_to_text(caption_full_dict: Mapping[str, Any]) -> str:\n",
    "    return RE_MULTIPLE_SPACES.sub(\" \", \" \".join(d[\"alternatives\"][0][\"transcript\"].strip()\n",
    "                                                for d in caption_full_dict[\"results\"][:-1])).strip()\n",
    "\n",
    "\n",
    "def _parse_caption_time(s: str) -> float:\n",
    "    return float(s[:-1])\n",
    "\n",
    "\n",
    "def _load_caption(path: str) -> Optional[Mapping[str, Any]]:\n",
    "    with open(path) as file:\n",
    "        caption_full_dict = json.load(file)\n",
    "\n",
    "        if results := caption_full_dict[\"results\"]:\n",
    "            tokens_info = results[-1][\"alternatives\"][0][\"words\"]\n",
    "        else:\n",
    "            tokens_info = None\n",
    "\n",
    "        if tokens_info:\n",
    "            return {  # Save some memory by just keeping what we actually use.\n",
    "                \"text\": _captions_to_text(caption_full_dict),\n",
    "                \"video_id\": os.path.basename(path).rsplit(\".\", maxsplit=1)[0],\n",
    "                \"tokens_info\": [{\n",
    "                    \"word\": wi[\"word\"],\n",
    "                    \"start_time\": _parse_caption_time(wi[\"startTime\"]),\n",
    "                    \"end_time\": _parse_caption_time(wi[\"endTime\"]),\n",
    "                } for wi in tokens_info],\n",
    "            }\n",
    "        else:\n",
    "            return None  # There are around 750/150k that fall here for different reasons.\n",
    "\n",
    "        \n",
    "def _add_caption_info_to_doc(doc: Doc, tokens_info: Sequence[Mapping[str, Any]]) -> Doc:\n",
    "    spacy2caption = spacy_alignments.get_alignments([t.text for t in doc], [w[\"word\"] for w in tokens_info])[0]\n",
    "\n",
    "    for token, caption_token_indices in zip(doc, spacy2caption):\n",
    "        token._.start_time = tokens_info[caption_token_indices[0]][\"start_time\"]\n",
    "        token._.end_time = tokens_info[caption_token_indices[-1]][\"end_time\"]\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a04db-9163-4da5-adf3-d2fe955de729",
   "metadata": {},
   "outputs": [],
   "source": [
    "Token.set_extension(\"start_time\", default=None)\n",
    "Token.set_extension(\"end_time\", default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997f25e-0638-4949-a91a-c6354fe3f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_to_doc(video_id: str) -> Doc:\n",
    "    caption = _load_caption(os.path.join(CAPTIONS_DIR, f\"{video_id}.json\"))\n",
    "    doc = nlp(caption[\"text\"])\n",
    "    return _add_caption_info_to_doc(doc, caption[\"tokens_info\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac2a32-fbcf-4e07-b7d1-9dbc3bf9c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = caption_to_doc(video_info[\"video_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733abf28-e3e7-4bbc-b445-d626f4fa214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def get_sents(doc: Doc) -> Iterator[Union[Span, str]]:\n",
    "    return doc.sents\n",
    "\n",
    "\n",
    "def get_noun_chunks(doc: Doc) -> Iterator[Union[Span, str]]:\n",
    "    for chunk in doc.noun_chunks:\n",
    "        yield f\"A photo of {chunk}.\"\n",
    "\n",
    "\n",
    "def get_verb_phrases(doc: Doc) -> Iterator[Union[Span, str]]:\n",
    "    for t in doc:\n",
    "        if t.pos_ == \"VERB\":\n",
    "            subtree = list(t.subtree)\n",
    "            yield doc[subtree[0].i:subtree[-1].i + 1]\n",
    "\n",
    "\n",
    "def get_orders(doc: Doc) -> Iterator[Union[Span, str]]:\n",
    "    for sent in doc.sents:\n",
    "        if sent[-1].text != \"?\":\n",
    "            for t in sent:\n",
    "                if (t.tag_ == \"VB\"\n",
    "                    and t.lower_ not in {\"know\", \"let\", \"try\"}\n",
    "                    and all(c.dep_ != \"aux\" for c in t.children)\n",
    "                    and t.dep_ not in {\"auxpass\", \"xcomp\"}):\n",
    "                    subtree = list(t.subtree)\n",
    "                    yield doc[subtree[0].i:subtree[-1].i + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a352144d-63c6-4cb9-806f-979b77ca91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "\n",
    "def show_caption_figures_and_pdf(video_id: str, doc: Doc, encoded_frames: torch.Tensor, clip_model: CLIP,\n",
    "                                 times: Sequence[float], thumbnail_times: Sequence[float],\n",
    "                                 thumbnails: Iterable[PIL.Image.Image], text_mode: str = \"sents\") -> None:\n",
    "    it = {\n",
    "        \"sents\": get_sents,\n",
    "        \"nouns\": get_noun_chunks,\n",
    "        \"verb_phrases\": get_verb_phrases,\n",
    "        \"orders\": get_orders,\n",
    "    }[text_mode](doc)\n",
    "\n",
    "    with PdfPages(f\"{video_id}.pdf\") as pdf_pages:\n",
    "        for text in tqdm(list(it)):\n",
    "            create_figure_for_text(encoded_frames, text, clip_model, times, thumbnail_times, thumbnails)\n",
    "            pdf_pages.savefig(bbox_inches=\"tight\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a8edd3-1db7-4f0e-8547-439cda28ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_figure_for_text(encoded_frames, next(iter(doc.sents)), clip_model, video_info[\"frame_times\"], video_info[\"thumbnail_times\"], video_info[\"thumbnails\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9465dd2a-6815-4bff-9194-892fd7b5adc5",
   "metadata": {},
   "source": [
    "## Changing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f590f8-54e6-4b6a-b622-ebe804e24106",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test = nlp(\"It penetrates all seven layers of skin and takes other nutrients deeper into hair follicles because it closely resembles human, skin composition emu, oil blocks DHT dihydrotestosterone, a male hormone, which loves to shrink hair, follicles, 90% of cases of male pattern, baldness occur due to the effect of DHT on hair follicles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0ffdde-3b24-4e50-9fde-aab16773e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "[t for t in doc_test if t.dep_ == \"cc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9425ef76-863f-4d7f-bfc2-b040b06c8c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = list(doc_test.sents)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87bfd34-a195-4e73-9b37-1fb0bc77c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sent.root.text, sent.root.dep_, sent.root.pos_, sent.root.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb0774a-daa7-4e53-a6ab-97548674b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\" \".join(t.text for t in c.subtree) for c in sent.root.children]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9cae1d-bed4-4e31-ade6-501ba6e2df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(c.text, c.dep_, c.pos_, c.tag_) for c in sent.root.children]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7dfd8-9335-44ad-90f6-55e3b197a4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\" \".join(t.text for t in c.subtree) for c in sent if c.pos_ == \"VERB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1071df8-c470-4bb7-affe-45ef65527c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec887356-f38e-4549-8c50-388b315dc873",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test2 = nlp(\"Almond oil has oleic acid omega-9 68% and vitamin K oleic acid opens pores and hair follicles to receive nutrients.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838080c-969a-4e71-9971-044682cc9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(doc_test2.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c1b56-70b5-43f7-9663-1e205c15debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(\"Shake it really well.\"), options={\"fine_grained\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cab4548-c9b0-41be-8a00-bc40ef42e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(\"They wake up and eat breakfast.\"), options={\"fine_grained\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400a5a6-30d7-4ac0-83c2-ec8f38e3f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(\"They have to do it.\"), options={\"fine_grained\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11eebb6-8733-4a98-8cff-3d8db47c2080",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(\"You will have to do it.\"), options={\"fine_grained\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80a656-a3e1-42cb-9f9d-2251c8ab390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(\"You'll have to do it.\"), options={\"fine_grained\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f607fbb7-8017-4ce5-8d2d-6abc3f7eed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "[sent for sent in doc.sents if sent.root.tag_ == \"VB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a1642-a1fc-4f52-a11d-8f92d459bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(\"Vitamin helps regulate good.\"), options={\"fine_grained\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a45b5ae-85d6-4a36-abbc-eac00449ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "[sent\n",
    " for sent in doc.sents\n",
    " if any(t.tag_ == \"VB\"\n",
    "        and all(c.lower_ not in {\"to\", \"will\", \"'ll'\"} for c in t.children)\n",
    "        and t.dep_ != \"xcomp\"\n",
    "        for t in sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4361c035-50d0-4b00-810f-ab8e79f1cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(\"It is highly concentrated so it can clog pores and must be mixed with other oils to be beneficial.\"), options={\"fine_grained\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78884737-b96e-4ac6-872b-7cbe78ced3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "[sent\n",
    " for sent in doc.sents\n",
    " if any(t.tag_ == \"VB\"\n",
    "        and all(c.lower_ not in {\"to\", \"will\", \"'ll'\"} for c in t.children)\n",
    "        and t.dep_ not in {\"auxpass\", \"xcomp\"}\n",
    "        for t in sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7581a0-9099-4521-8328-8f6b96070bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More radical; with no auxiliaries.\n",
    "[sent\n",
    " for sent in doc.sents\n",
    " if any(t.tag_ == \"VB\"\n",
    "        and all(c.dep_ != \"aux\" for c in t.children)\n",
    "        and t.dep_ not in {\"auxpass\", \"xcomp\"}\n",
    "        for t in sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b8e56e-fffc-49e6-aa07-06638c77da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(\"Is it makes skin on scalp grow thicker and stronger which holds the hair tightly in place?\"), options={\"fine_grained\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "376392b8-ccb3-4734-9d3a-9bc48bdd6a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Add Brewster Home Fashions, we design wallpapers that are easy to live with.,\n",
       " Thanks to paste the wall technology, it is surprisingly easy to install and remove modern wallpaper in just a matter of hours.,\n",
       " Paste paste brush or roller knife or snap off blade level sponge and bucket of Clean.,\n",
       " Water, smoothing brush or plastic smoother tape measure 4 inch to 6 inch.,\n",
       " And or straight edge a pencil step stool and finally turn off your cell phone and turn on the music.,\n",
       " First, you need to make sure the walls are clean of all debris. And that the surface is smooth, spackle and smooth out any holes or rough areas on the walls as they may affect the final finish of your wallpaper.,\n",
       " So it is important to take your time and do this first step correctly to begin.,\n",
       " You want to start by marking a guideline on the wall for the proper placement of your first strip select, your starting point and measure over one inch less than the width of your wall paper roll.,\n",
       " Mark the Spot with pencil in the middle of the wall and just below eye level.,\n",
       " Now, take out your level and draw a three to four foot, vertical, line with your pencil.,\n",
       " We now need to measure the length for the strip's, take your roll of wallpaper and measure the length of your Strip against the wall.,\n",
       " It is important that you take into consideration the design of your wallpaper while doing this generally speaking.,\n",
       " So make your best determination when measuring the length of the wall paper strips, be sure to allow a minimum of two.,\n",
       " And finally, taking your super sharp knife blade or scissors, cut your first strip of wallpaper.,\n",
       " It is important to get this first strip centered and leveled perfectly as you will be hanging the rest of the strips.,\n",
       " Take your brush and paste enough of the wall to accommodate the first strip of wallpaper next line, the edge of your strip against the guide line.,\n",
       " Once you have lined up the wall paper strip to your liking, smooth it out using the smoothing brush or Smoother to make sure.,\n",
       " There are no bubbles or ripples using your knife, trim.,\n",
       " Always use a sharp knife or blade for every cut, try not to lift your knife.,\n",
       " , move the hard Edge and then follow with the knife.,\n",
       " Take a wet clean sponge and wipe off any glue residue from the front of the wall paper strip.,\n",
       " Remember to only use clean water onto your next strip, if you have a pattern repeat, you will also need to match the next strip against the first one.,\n",
       " Take the roll of wallpaper and hold At the top and unroll it down the wall.,\n",
       " In order to measure the length again, be sure to start the second strip in a place that matches the pattern of the first strip.,\n",
       " Repeat the process until the wall is covered.,\n",
       " When approaching a corner, hang the final strip on the current wall, which will overlap the corner onto the new wall, take your sharp knife and trim the wallpaper down the corner of the wall.,\n",
       " Remember to always use a sharp knife or blade for every cut and to know.,\n",
       " Lift your knife.,\n",
       " If it isn't take a new strip of wallpaper and match the pattern, as you did your previous strip when starting a new wall, be sure that the first strip is level.,\n",
       " Next, remove the plate from the wall.,\n",
       " When you hang the wallpaper, the allowed the size of the electrical box underneath and cut out the size of the Box.,\n",
       " Put played back on and you are good to go.,\n",
       " When cutting measuring your strips, remember is always to continue to match your pattern.,\n",
       " Hang your strip from the top for easier installation.,\n",
       " Take your scissors or a sharp blade and create a relief by locating, the corner of the door frame, and cut at a 45 degree, angle towards the center of the door.,\n",
       " Take your straight edge and trim. Using a new Blade, the top first, and then the side haste and attaches usual, repeat the same process for the other side.,\n",
       " Step back and admire your beautifully.,\n",
       " Wallpapered room move the furniture back into place, sit down, relax and enjoy the ambience of the style you have created please.,\n",
       " Visit w-w-w dot Brewster Home Fashions.com to view our entire line of wallpapers and Home Decor products.]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sent\n",
    " for sent in doc.sents\n",
    " if any(t.tag_ == \"VB\"\n",
    "        and t.lower_ != \"try\"\n",
    "        and all(c.dep_ != \"aux\" for c in t.children)\n",
    "        and t.dep_ not in {\"auxpass\", \"xcomp\"}\n",
    "        for t in sent)\n",
    " and sent[-1].text != \"?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db79baa-41f3-4ae5-932e-d691c8a04150",
   "metadata": {},
   "source": [
    "## Visualizing Multiple Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f0d0c-b9e2-47ed-b87c-c20fb16dbebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_caption_figures_and_pdf(video_info[\"video_id\"], doc, encoded_frames, video_info[\"frame_times\"], video_info[\"thumbnail_times\"], video_info[\"thumbnails\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7b9e8-0e77-4902-913b-f1145fa12afa",
   "metadata": {},
   "source": [
    "Just some of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79279293-b90e-4155-ace2-35b202263752",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_caption_figures_and_pdf(video_info[\"video_id\"], doc, encoded_frames, video_info[\"frame_times\"], video_info[\"thumbnail_times\"], video_info[\"thumbnails\"], text_mode=\"orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4124fe-3eaa-42c3-8c5b-81a13d7b5434",
   "metadata": {},
   "source": [
    "## Another Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4603aa2f-a07d-4e81-bd9b-8913b352ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "video2_info = get_local_video_info(\"2xVpyPnxg9c\")\n",
    "encoded_frames2 = encode_visual(video2_info[\"frames\"], clip_model, image_preprocessor, device=device)\n",
    "\n",
    "doc2 = caption_to_doc(video2_info[\"video_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3208c-49f8-4b26-85a7-42b415f24a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_caption_figures_and_pdf(video2_info[\"video_id\"], doc2, encoded_frames2, video2_info[\"frame_times\"], video2_info[\"thumbnail_times\"], video2_info[\"thumbnails\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6c275-7aad-4709-8171-0af8ab562ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_caption_figures_and_pdf(video2_info[\"video_id\"], doc2, encoded_frames2, video2_info[\"frame_times\"], video2_info[\"thumbnail_times\"], video2_info[\"thumbnails\"], text_mode=\"orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7e1a27-f66d-4198-a529-0c29da1dff60",
   "metadata": {},
   "source": [
    "## 1st vs 3rd person in the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1b3bbf-b1ba-418f-a8e8-e86e6878136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_figure_for_text(encoded_frames, \"I'm pouring this liquid into the container.\", clip_model, video_info[\"frame_times\"], video_info[\"thumbnail_times\"], video_info[\"thumbnails\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de611887-575f-43a6-b979-d8356176ebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_figure_for_text(encoded_frames, \"He's pouring the liquid into a container.\", clip_model, video_info[\"frame_times\"], video_info[\"thumbnail_times\"], video_info[\"thumbnails\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f16210-2a0f-4e9a-9a2f-34b663d028f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "video_ids = []\n",
    "with os.scandir(\"demo/static/videos/\") as it:\n",
    "    for entry in it:\n",
    "        if entry.is_file() and entry.name.endswith((\".mp4\", \".webm\")):\n",
    "            video_ids.append(entry.name.rsplit(\".\", maxsplit=1)[0])\n",
    "\n",
    "selected_video_ids = random.sample(video_ids, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a61ba6-aa2a-4728-808b-4493c1130010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "video_infos = []\n",
    "encoded_frames_list = []\n",
    "doc_list = []\n",
    "\n",
    "for video_id in tqdm(selected_video_ids):\n",
    "    try:\n",
    "        video_info = get_local_video_info(video_id)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "    encoded_frames = encode_visual(video_info[\"frames\"], clip_model, image_preprocessor, device=device)\n",
    "\n",
    "    doc = caption_to_doc(video_info[\"video_id\"])\n",
    "    \n",
    "    video_infos.append(video_info)\n",
    "    encoded_frames_list.append(encoded_frames)\n",
    "    doc_list.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d48f180-dfa0-486d-8570-e50f70cd593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(video_infos)))\n",
    "\n",
    "selected = random.choices(indices, k=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa69f66-9fae-44a0-9be3-f39ef3e39b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(\"random.pdf\") as pdf_pages:\n",
    "    for i in tqdm(selected):\n",
    "        video_info = video_infos[i]\n",
    "        encoded_frames = encoded_frames_list[i]\n",
    "        doc = doc_list[i]\n",
    "\n",
    "        sents = list(doc.sents)\n",
    "        sent = random.choice(sents)\n",
    "        create_figure_for_text(encoded_frames, sent, clip_model, video_info[\"frame_times\"], video_info[\"thumbnail_times\"], video_info[\"thumbnails\"])\n",
    "        pdf_pages.savefig(bbox_inches=\"tight\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbee9327-87e3-4e8f-bd75-15bef3c57904",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_list = [list(get_orders(doc)) for doc in doc_list]\n",
    "\n",
    "with PdfPages(\"orders.pdf\") as pdf_pages:\n",
    "    for i in tqdm(selected):\n",
    "        video_info = video_infos[i]\n",
    "        encoded_frames = encoded_frames_list[i]\n",
    "        \n",
    "        orders = orders_list[i]\n",
    "        order = random.choice(orders)\n",
    "        \n",
    "        create_figure_for_text(encoded_frames, order, clip_model, video_info[\"frame_times\"], video_info[\"thumbnail_times\"], video_info[\"thumbnails\"])\n",
    "        pdf_pages.savefig(bbox_inches=\"tight\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82b1cc-1f90-4992-a87b-01d516fca4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_phrases_list = [list(get_verb_phrases(doc)) for doc in doc_list]\n",
    "\n",
    "with PdfPages(\"clauses.pdf\") as pdf_pages:\n",
    "    for i in tqdm(selected):\n",
    "        video_info = video_infos[i]\n",
    "        encoded_frames = encoded_frames_list[i]\n",
    "        \n",
    "        verb_phrases = verb_phrases_list[i]\n",
    "        vp = random.choice(verb_phrases)\n",
    "        \n",
    "        create_figure_for_text(encoded_frames, vp, clip_model, video_info[\"frame_times\"], video_info[\"thumbnail_times\"], video_info[\"thumbnails\"])\n",
    "        pdf_pages.savefig(bbox_inches=\"tight\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
